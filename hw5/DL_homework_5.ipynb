{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":94008,"databundleVersionId":11187445,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom typing import Callable\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split\nfrom torchvision import transforms\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader, Subset\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom tqdm import trange\nfrom tqdm.auto import tqdm\n\nfrom IPython.display import clear_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:12:21.537162Z","iopub.execute_input":"2025-03-04T23:12:21.537530Z","iopub.status.idle":"2025-03-04T23:12:31.771421Z","shell.execute_reply.started":"2025-03-04T23:12:21.537498Z","shell.execute_reply":"2025-03-04T23:12:31.770763Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nfrom torch.utils.data import WeightedRandomSampler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:12:34.003859Z","iopub.execute_input":"2025-03-04T23:12:34.004120Z","iopub.status.idle":"2025-03-04T23:12:34.375096Z","shell.execute_reply.started":"2025-03-04T23:12:34.004100Z","shell.execute_reply":"2025-03-04T23:12:34.374429Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from sklearn.model_selection import KFold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:12:35.141233Z","iopub.execute_input":"2025-03-04T23:12:35.141678Z","iopub.status.idle":"2025-03-04T23:12:35.368009Z","shell.execute_reply.started":"2025-03-04T23:12:35.141652Z","shell.execute_reply":"2025-03-04T23:12:35.367395Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"MODEL_NAME = \"efficientnet_b4_tuned\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_LABELS_PATH = \"/kaggle/input/dl-5-image-classification/train-labels.csv\"\nIMG_TRAIN_DIR = \"/kaggle/input/dl-5-image-classification/train\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:21:03.160864Z","iopub.execute_input":"2025-03-02T22:21:03.161053Z","iopub.status.idle":"2025-03-02T22:21:03.164234Z","shell.execute_reply.started":"2025-03-02T22:21:03.161037Z","shell.execute_reply":"2025-03-02T22:21:03.163433Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"MODEL_OUTPUT_PATH = f\"/kaggle/working/{MODEL_NAME}.pth\"\nLOGS_PATH = f\"/kaggle/working/{MODEL_NAME}-logs.npy\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 64\nNUM_EPOCHS = 100\nPATIENCE = 10\nNUM_CLASSES = 20\nLEARNING_RATE = 1e-4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = pd.read_csv(TRAIN_LABELS_PATH)\ndf_train = df_train[df_train[\"image\"] != \"39401.jpg\"]\ndf_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:21:11.493534Z","iopub.execute_input":"2025-03-02T22:21:11.493846Z","iopub.status.idle":"2025-03-02T22:21:11.524154Z","shell.execute_reply.started":"2025-03-02T22:21:11.493818Z","shell.execute_reply":"2025-03-02T22:21:11.523445Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"           image         label\n0      21310.jpg    Sunglasses\n1       8993.jpg  Sports Shoes\n2      33363.jpg       Wallets\n3      27744.jpg      Handbags\n4      11603.jpg        Kurtas\n...          ...           ...\n16571  57536.jpg         Belts\n16572  56337.jpg       Watches\n16573  29762.jpg       Watches\n16574  11515.jpg         Heels\n16575  13064.jpg  Casual Shoes\n\n[16575 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>21310.jpg</td>\n      <td>Sunglasses</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8993.jpg</td>\n      <td>Sports Shoes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>33363.jpg</td>\n      <td>Wallets</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>27744.jpg</td>\n      <td>Handbags</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11603.jpg</td>\n      <td>Kurtas</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16571</th>\n      <td>57536.jpg</td>\n      <td>Belts</td>\n    </tr>\n    <tr>\n      <th>16572</th>\n      <td>56337.jpg</td>\n      <td>Watches</td>\n    </tr>\n    <tr>\n      <th>16573</th>\n      <td>29762.jpg</td>\n      <td>Watches</td>\n    </tr>\n    <tr>\n      <th>16574</th>\n      <td>11515.jpg</td>\n      <td>Heels</td>\n    </tr>\n    <tr>\n      <th>16575</th>\n      <td>13064.jpg</td>\n      <td>Casual Shoes</td>\n    </tr>\n  </tbody>\n</table>\n<p>16575 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"labels = df_train.label.unique()\nlabels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:21:11.741098Z","iopub.execute_input":"2025-03-02T22:21:11.741390Z","iopub.status.idle":"2025-03-02T22:21:11.747471Z","shell.execute_reply.started":"2025-03-02T22:21:11.741335Z","shell.execute_reply":"2025-03-02T22:21:11.746747Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array(['Sunglasses', 'Sports Shoes', 'Wallets', 'Handbags', 'Kurtas',\n       'Casual Shoes', 'Shirts', 'Perfume and Body Mist', 'Tshirts',\n       'Tops', 'Flip Flops', 'Backpacks', 'Watches', 'Belts',\n       'Formal Shoes', 'Socks', 'Jeans', 'Heels', 'Sandals', 'Briefs'],\n      dtype=object)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"map_label_to_int = {}\nmap_int_to_label = {}\n\nfor i, label in enumerate(labels):\n    map_label_to_int[label] = i\n    map_int_to_label[i] = label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:21:14.103814Z","iopub.execute_input":"2025-03-02T22:21:14.104094Z","iopub.status.idle":"2025-03-02T22:21:14.108310Z","shell.execute_reply.started":"2025-03-02T22:21:14.104072Z","shell.execute_reply":"2025-03-02T22:21:14.107395Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df_train[\"label_int\"] = df_train[\"label\"].map(map_label_to_int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:21:14.320568Z","iopub.execute_input":"2025-03-02T22:21:14.320791Z","iopub.status.idle":"2025-03-02T22:21:14.326994Z","shell.execute_reply.started":"2025-03-02T22:21:14.320772Z","shell.execute_reply":"2025-03-02T22:21:14.326199Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:21:14.497633Z","iopub.execute_input":"2025-03-02T22:21:14.497916Z","iopub.status.idle":"2025-03-02T22:21:14.507365Z","shell.execute_reply.started":"2025-03-02T22:21:14.497890Z","shell.execute_reply":"2025-03-02T22:21:14.506445Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"           image         label  label_int\n0      21310.jpg    Sunglasses          0\n1       8993.jpg  Sports Shoes          1\n2      33363.jpg       Wallets          2\n3      27744.jpg      Handbags          3\n4      11603.jpg        Kurtas          4\n...          ...           ...        ...\n16571  57536.jpg         Belts         13\n16572  56337.jpg       Watches         12\n16573  29762.jpg       Watches         12\n16574  11515.jpg         Heels         17\n16575  13064.jpg  Casual Shoes          5\n\n[16575 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>label</th>\n      <th>label_int</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>21310.jpg</td>\n      <td>Sunglasses</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8993.jpg</td>\n      <td>Sports Shoes</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>33363.jpg</td>\n      <td>Wallets</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>27744.jpg</td>\n      <td>Handbags</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11603.jpg</td>\n      <td>Kurtas</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16571</th>\n      <td>57536.jpg</td>\n      <td>Belts</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>16572</th>\n      <td>56337.jpg</td>\n      <td>Watches</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>16573</th>\n      <td>29762.jpg</td>\n      <td>Watches</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>16574</th>\n      <td>11515.jpg</td>\n      <td>Heels</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>16575</th>\n      <td>13064.jpg</td>\n      <td>Casual Shoes</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>16575 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Класс для работы с кастомным датасетом","metadata":{}},{"cell_type":"code","source":"class FashionDataset(Dataset):\n    def __init__(self, df_meta: pd.DataFrame, img_dir: str, transform: Callable=None):\n        super().__init__()\n        self.img_labels = df_meta\n        self.img_dir = img_dir\n        self.transform = transform\n                                    \n    def __getitem__(self, idx: int) -> (torch.tensor, torch.tensor):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        image = Image.open(img_path)\n        label = self.img_labels.iloc[idx, 2]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n        \n\n    def __len__(self) -> int:\n        return len(self.img_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:21:16.610860Z","iopub.execute_input":"2025-03-02T22:21:16.611211Z","iopub.status.idle":"2025-03-02T22:21:16.617114Z","shell.execute_reply.started":"2025-03-02T22:21:16.611178Z","shell.execute_reply":"2025-03-02T22:21:16.616328Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Трансформации данных","metadata":{}},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406], \n        std=[0.229, 0.224, 0.225]\n    )\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_dataset = FashionDataset(\n    df_meta = df_train,\n    img_dir = IMG_TRAIN_DIR,\n    transform = None\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_size = int(0.8 * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_subset, val_subset = random_split(full_dataset, [train_size, val_size])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_subset.dataset.transform = train_transform\nval_subset.dataset.transform = val_transform","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Training on: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = [label for _, label in train_subset]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_counts = np.bincount(labels)\n\nsample_weights = 1.0 / class_counts[labels]\nsample_weights = torch.tensor(sample_weights, dtype=torch.float)\n\nsampler = WeightedRandomSampler(sample_weights, len(sample_weights))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    train_subset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=4,\n    sampler=sampler\n)\n\nval_dataloader = DataLoader(\n    val_subset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=4\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = models.efficientnet_b4(pretrained=True)\n# model.classifier[1] = nn.Linear(model.classifier[1].in_features, NUM_CLASSES)\n\nmodel = models.resnet50(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.device_count()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.to(device)\n\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_val_loss = float(\"inf\")\npatience = 10\ntraining_time_start = time.time()\n\nval_loss_logs = []\ntrain_loss_logs = []\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    running_loss = 0.0\n    for images, labels in train_dataloader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step() \n        running_loss += loss.item()\n\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for images, labels in val_dataloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n    train_loss = running_loss / len(train_dataloader)\n    val_loss = val_loss / len(val_dataloader)\n\n    scheduler.step(val_loss)\n    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']}\")\n\n    train_loss_logs.append(train_loss)\n    val_loss_logs.append(val_loss)\n    \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'val_loss': val_loss,\n        }, MODEL_OUTPUT_PATH)\n        patience = 10\n    else:\n        patience -= 1\n        if patience == 0:\n            print(\"Early stopping\")\n            break\n\ntraining_time_end = time.time()\ntraining_time = training_time_end - training_time_start\n\nlogs = {\n    \"val_loss\": val_loss_logs,\n    \"train_loss\": train_loss_logs,\n    \"learning_time\": training_time\n}\nnp.save(LOGS_PATH, logs)\n\nprint(f\"Training complete! Training time: {training_time}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Обучение с K-Fold Cross Validation","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T11:20:55.469077Z","iopub.execute_input":"2025-03-03T11:20:55.469363Z","iopub.status.idle":"2025-03-03T11:20:55.550778Z","shell.execute_reply.started":"2025-03-03T11:20:55.469342Z","shell.execute_reply":"2025-03-03T11:20:55.549751Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### Гиперпараметры","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 64\nNUM_EPOCHS = 100\nPATIENCE = 10\nNUM_CLASSES = 20\nLEARNING_RATE = 1e-4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:21:19.971776Z","iopub.execute_input":"2025-03-02T22:21:19.972063Z","iopub.status.idle":"2025-03-02T22:21:19.975919Z","shell.execute_reply.started":"2025-03-02T22:21:19.972039Z","shell.execute_reply":"2025-03-02T22:21:19.975007Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Функция, возвращающая предобученную модель","metadata":{}},{"cell_type":"code","source":"def create_model():\n    model = models.efficientnet_b4(\n        weights=models.EfficientNet_B4_Weights.IMAGENET1K_V1\n    )\n    model.classifier[1] = nn.Linear(model.classifier[1].in_features, NUM_CLASSES)\n\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n    model.to(device)\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:21:20.312563Z","iopub.execute_input":"2025-03-02T22:21:20.312951Z","iopub.status.idle":"2025-03-02T22:21:20.317545Z","shell.execute_reply.started":"2025-03-02T22:21:20.312919Z","shell.execute_reply":"2025-03-02T22:21:20.316576Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### Инициализация переменной для работы с кастомным датасетом","metadata":{}},{"cell_type":"code","source":"dataset = FashionDataset(\n    df_meta = df_train,\n    img_dir = IMG_TRAIN_DIR,\n    transform = None\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:21:20.573082Z","iopub.execute_input":"2025-03-02T22:21:20.573451Z","iopub.status.idle":"2025-03-02T22:21:20.577513Z","shell.execute_reply.started":"2025-03-02T22:21:20.573419Z","shell.execute_reply":"2025-03-02T22:21:20.576397Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Трансформации для тренировочного и валидационного набора данных","metadata":{}},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406], \n        std=[0.229, 0.224, 0.225]\n    ),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:21:20.747485Z","iopub.execute_input":"2025-03-02T22:21:20.747765Z","iopub.status.idle":"2025-03-02T22:21:20.752709Z","shell.execute_reply.started":"2025-03-02T22:21:20.747742Z","shell.execute_reply":"2025-03-02T22:21:20.751711Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"val_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406], \n        std=[0.229, 0.224, 0.225]\n    )\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:21:23.605474Z","iopub.execute_input":"2025-03-02T22:21:23.605762Z","iopub.status.idle":"2025-03-02T22:21:23.610015Z","shell.execute_reply.started":"2025-03-02T22:21:23.605738Z","shell.execute_reply":"2025-03-02T22:21:23.609196Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:27:13.556532Z","iopub.execute_input":"2025-03-02T22:27:13.556820Z","iopub.status.idle":"2025-03-02T22:27:13.560291Z","shell.execute_reply.started":"2025-03-02T22:27:13.556799Z","shell.execute_reply":"2025-03-02T22:27:13.559491Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### Цикл обучения","metadata":{}},{"cell_type":"code","source":"def train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, fold):\n    best_val_loss = float(\"inf\")\n    patience = 10\n    training_time_start = time.time()\n\n    val_loss_logs = []\n    train_loss_logs = []\n    for epoch in range(NUM_EPOCHS):\n        model.train()\n        running_loss = 0.0\n        for images, labels in train_dataloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step() \n            running_loss += loss.item()\n\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for images, labels in val_dataloader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n        train_loss = running_loss / len(train_dataloader)\n        val_loss = val_loss / len(val_dataloader)\n\n        scheduler.step(val_loss)\n        print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']}\")\n\n        train_loss_logs.append(train_loss)\n        val_loss_logs.append(val_loss)\n    \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), f\"/kaggle/working/efficientnet_b4-fold-{fold+1}.pth\")\n            patience = 10\n        else:\n            patience -= 1\n            if patience == 0:\n                print(\"Early stopping\")\n                break\n\n    training_time_end = time.time()\n    training_time = training_time_end - training_time_start\n\n    logs = {\n        \"val_loss\": val_loss_logs,\n        \"train_loss\": train_loss_logs,\n        \"learning_time\": training_time\n    }\n    np.save(f\"/kaggle/working/logs-fold-{fold}\", logs)\n\n    return best_val_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:27:17.740615Z","iopub.execute_input":"2025-03-02T22:27:17.740913Z","iopub.status.idle":"2025-03-02T22:27:17.749253Z","shell.execute_reply.started":"2025-03-02T22:27:17.740892Z","shell.execute_reply":"2025-03-02T22:27:17.748224Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Главный цикл K-Fold Cross-Validation","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import KFold\n\nk = 5\nkfold = KFold(n_splits=k, shuffle=True, random_state=42)\n\nfold_results = []\nfor fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n    print(f\"Fold {fold+1}\")\n    \n    train_subset = Subset(dataset, train_ids)\n    val_subset = Subset(dataset, val_ids)\n\n    train_subset.dataset.transform = train_transform\n    val_subset.dataset.transform = val_transform\n\n    train_labels = [label for _, label in train_subset]\n    class_counts = torch.bincount(torch.tensor(train_labels))\n    class_weights = 1. / class_counts\n    class_weights = class_weights / class_weights.sum()        \n    sample_weights = class_weights[train_labels]\n    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n\n    model = create_model()\n    train_dataloader = DataLoader(\n        train_subset, \n        batch_size=BATCH_SIZE, \n        sampler=sampler\n    )\n    val_dataloader = DataLoader(\n        val_subset, \n        batch_size=BATCH_SIZE, \n        shuffle=False\n    )\n    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n\n    val_loss = train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, fold)\n    fold_results.append(val_loss)\n\nprint(f\"Fold results: {fold_results}\")\nprint(f\"Average validation loss: {sum(fold_results)/len(fold_results)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:27:30.513320Z","iopub.execute_input":"2025-03-02T22:27:30.513680Z","execution_failed":"2025-03-03T10:05:13.086Z"}},"outputs":[{"name":"stdout","text":"Fold 1\nEpoch [1/100], Train Loss: 1.2001, Val Loss: 0.3365, LR: 0.0001\nEpoch [2/100], Train Loss: 0.2123, Val Loss: 0.2300, LR: 0.0001\nEpoch [3/100], Train Loss: 0.1380, Val Loss: 0.2039, LR: 0.0001\nEpoch [4/100], Train Loss: 0.1003, Val Loss: 0.1858, LR: 0.0001\nEpoch [5/100], Train Loss: 0.0782, Val Loss: 0.1867, LR: 0.0001\nEpoch [6/100], Train Loss: 0.0602, Val Loss: 0.1779, LR: 0.0001\nEpoch [7/100], Train Loss: 0.0567, Val Loss: 0.1742, LR: 0.0001\nEpoch [8/100], Train Loss: 0.0469, Val Loss: 0.1851, LR: 0.0001\nEpoch [9/100], Train Loss: 0.0374, Val Loss: 0.2077, LR: 0.0001\nEpoch [10/100], Train Loss: 0.0345, Val Loss: 0.1866, LR: 0.0001\nEpoch [11/100], Train Loss: 0.0320, Val Loss: 0.1920, LR: 0.0001\nEpoch [12/100], Train Loss: 0.0291, Val Loss: 0.2010, LR: 0.0001\nEpoch [13/100], Train Loss: 0.0261, Val Loss: 0.1999, LR: 1e-05\nEpoch [14/100], Train Loss: 0.0216, Val Loss: 0.1922, LR: 1e-05\nEpoch [15/100], Train Loss: 0.0224, Val Loss: 0.1896, LR: 1e-05\nEpoch [16/100], Train Loss: 0.0223, Val Loss: 0.1944, LR: 1e-05\nEpoch [17/100], Train Loss: 0.0190, Val Loss: 0.1916, LR: 1e-05\nEarly stopping\nFold 2\nEpoch [1/100], Train Loss: 1.1805, Val Loss: 0.3646, LR: 0.0001\nEpoch [2/100], Train Loss: 0.1938, Val Loss: 0.2215, LR: 0.0001\nEpoch [3/100], Train Loss: 0.1330, Val Loss: 0.1997, LR: 0.0001\nEpoch [4/100], Train Loss: 0.1002, Val Loss: 0.1895, LR: 0.0001\nEpoch [5/100], Train Loss: 0.0798, Val Loss: 0.1868, LR: 0.0001\nEpoch [6/100], Train Loss: 0.0650, Val Loss: 0.1863, LR: 0.0001\nEpoch [7/100], Train Loss: 0.0547, Val Loss: 0.1872, LR: 0.0001\nEpoch [8/100], Train Loss: 0.0454, Val Loss: 0.1891, LR: 0.0001\nEpoch [9/100], Train Loss: 0.0377, Val Loss: 0.2017, LR: 0.0001\nEpoch [10/100], Train Loss: 0.0354, Val Loss: 0.2016, LR: 0.0001\nEpoch [11/100], Train Loss: 0.0344, Val Loss: 0.2037, LR: 0.0001\nEpoch [12/100], Train Loss: 0.0281, Val Loss: 0.1949, LR: 1e-05\nEpoch [13/100], Train Loss: 0.0245, Val Loss: 0.1995, LR: 1e-05\nEpoch [14/100], Train Loss: 0.0240, Val Loss: 0.1999, LR: 1e-05\nEpoch [15/100], Train Loss: 0.0232, Val Loss: 0.2084, LR: 1e-05\nEpoch [16/100], Train Loss: 0.0191, Val Loss: 0.2012, LR: 1e-05\nEarly stopping\nFold 3\nEpoch [1/100], Train Loss: 1.1754, Val Loss: 0.3577, LR: 0.0001\nEpoch [2/100], Train Loss: 0.1994, Val Loss: 0.2351, LR: 0.0001\nEpoch [3/100], Train Loss: 0.1383, Val Loss: 0.2080, LR: 0.0001\nEpoch [4/100], Train Loss: 0.1056, Val Loss: 0.1890, LR: 0.0001\nEpoch [5/100], Train Loss: 0.0768, Val Loss: 0.1804, LR: 0.0001\nEpoch [6/100], Train Loss: 0.0704, Val Loss: 0.1787, LR: 0.0001\nEpoch [7/100], Train Loss: 0.0558, Val Loss: 0.1685, LR: 0.0001\nEpoch [8/100], Train Loss: 0.0472, Val Loss: 0.1669, LR: 0.0001\nEpoch [9/100], Train Loss: 0.0412, Val Loss: 0.1800, LR: 0.0001\nEpoch [10/100], Train Loss: 0.0365, Val Loss: 0.1762, LR: 0.0001\nEpoch [11/100], Train Loss: 0.0322, Val Loss: 0.1758, LR: 0.0001\nEpoch [12/100], Train Loss: 0.0294, Val Loss: 0.1813, LR: 0.0001\nEpoch [13/100], Train Loss: 0.0258, Val Loss: 0.1842, LR: 0.0001\nEpoch [14/100], Train Loss: 0.0237, Val Loss: 0.1784, LR: 1e-05\nEpoch [15/100], Train Loss: 0.0223, Val Loss: 0.1784, LR: 1e-05\nEpoch [16/100], Train Loss: 0.0198, Val Loss: 0.1854, LR: 1e-05\nEpoch [17/100], Train Loss: 0.0212, Val Loss: 0.1796, LR: 1e-05\nEpoch [18/100], Train Loss: 0.0178, Val Loss: 0.1815, LR: 1e-05\nEarly stopping\nFold 4\nEpoch [1/100], Train Loss: 1.1857, Val Loss: 0.3399, LR: 0.0001\nEpoch [2/100], Train Loss: 0.1988, Val Loss: 0.2446, LR: 0.0001\nEpoch [3/100], Train Loss: 0.1388, Val Loss: 0.1899, LR: 0.0001\nEpoch [4/100], Train Loss: 0.1066, Val Loss: 0.1917, LR: 0.0001\nEpoch [5/100], Train Loss: 0.0842, Val Loss: 0.1888, LR: 0.0001\nEpoch [6/100], Train Loss: 0.0685, Val Loss: 0.1742, LR: 0.0001\nEpoch [7/100], Train Loss: 0.0560, Val Loss: 0.1766, LR: 0.0001\nEpoch [8/100], Train Loss: 0.0446, Val Loss: 0.1697, LR: 0.0001\nEpoch [9/100], Train Loss: 0.0417, Val Loss: 0.1706, LR: 0.0001\nEpoch [10/100], Train Loss: 0.0357, Val Loss: 0.1686, LR: 0.0001\nEpoch [11/100], Train Loss: 0.0303, Val Loss: 0.1795, LR: 0.0001\nEpoch [12/100], Train Loss: 0.0305, Val Loss: 0.1925, LR: 0.0001\nEpoch [13/100], Train Loss: 0.0269, Val Loss: 0.1826, LR: 0.0001\nEpoch [14/100], Train Loss: 0.0244, Val Loss: 0.1764, LR: 0.0001\nEpoch [15/100], Train Loss: 0.0212, Val Loss: 0.1786, LR: 0.0001\nEpoch [16/100], Train Loss: 0.0192, Val Loss: 0.1892, LR: 1e-05\nEpoch [17/100], Train Loss: 0.0179, Val Loss: 0.1943, LR: 1e-05\nEpoch [18/100], Train Loss: 0.0166, Val Loss: 0.1899, LR: 1e-05\nEpoch [19/100], Train Loss: 0.0164, Val Loss: 0.1907, LR: 1e-05\nEpoch [20/100], Train Loss: 0.0156, Val Loss: 0.1867, LR: 1e-05\nEarly stopping\nFold 5\nEpoch [1/100], Train Loss: 1.1637, Val Loss: 0.3518, LR: 0.0001\nEpoch [2/100], Train Loss: 0.1908, Val Loss: 0.2720, LR: 0.0001\nEpoch [3/100], Train Loss: 0.1494, Val Loss: 0.2103, LR: 0.0001\nEpoch [4/100], Train Loss: 0.1053, Val Loss: 0.1853, LR: 0.0001\nEpoch [5/100], Train Loss: 0.0740, Val Loss: 0.1977, LR: 0.0001\nEpoch [6/100], Train Loss: 0.0680, Val Loss: 0.1846, LR: 0.0001\nEpoch [7/100], Train Loss: 0.0540, Val Loss: 0.1741, LR: 0.0001\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"class TestFashionDataset(Dataset):\n    def __init__(self, df_images: pd.DataFrame, img_dir: str, transform: Callable=None):\n        super().__init__()\n        self.img_names = df_images\n        self.img_dir = img_dir\n        self.transform = transform\n                                    \n    def __getitem__(self, idx: int) -> (torch.tensor, torch.tensor):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_path = os.path.join(self.img_dir, self.img_names.iloc[idx, 0])\n        image = Image.open(img_path)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image\n        \n    def __len__(self) -> int:\n        return len(self.img_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T11:33:31.667255Z","iopub.execute_input":"2025-03-03T11:33:31.667564Z","iopub.status.idle":"2025-03-03T11:33:31.672815Z","shell.execute_reply.started":"2025-03-03T11:33:31.667540Z","shell.execute_reply":"2025-03-03T11:33:31.672073Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"df_images = pd.read_csv(\"/kaggle/input/dl-5-image-classification/sample_submission.csv\")\ndf_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T11:33:34.078896Z","iopub.execute_input":"2025-03-03T11:33:34.079173Z","iopub.status.idle":"2025-03-03T11:33:34.098335Z","shell.execute_reply.started":"2025-03-03T11:33:34.079152Z","shell.execute_reply":"2025-03-03T11:33:34.097484Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"           image  label\n0       4117.jpg    NaN\n1      48551.jpg    NaN\n2      18248.jpg    NaN\n3      45905.jpg    NaN\n4      44184.jpg    NaN\n...          ...    ...\n16569  41512.jpg    NaN\n16570  52106.jpg    NaN\n16571  27085.jpg    NaN\n16572  51680.jpg    NaN\n16573  33371.jpg    NaN\n\n[16574 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4117.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>48551.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18248.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>45905.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>44184.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16569</th>\n      <td>41512.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16570</th>\n      <td>52106.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16571</th>\n      <td>27085.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16572</th>\n      <td>51680.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16573</th>\n      <td>33371.jpg</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>16574 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"test_dataset = TestFashionDataset(\n    df_images = df_images,\n    img_dir = \"/kaggle/input/dl-5-image-classification/test/test\",\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:32:16.801343Z","iopub.execute_input":"2025-03-03T12:32:16.801637Z","iopub.status.idle":"2025-03-03T12:32:16.807680Z","shell.execute_reply.started":"2025-03-03T12:32:16.801606Z","shell.execute_reply":"2025-03-03T12:32:16.806881Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"test_dataloader = DataLoader(\n    dataset=test_dataset,\n    batch_size=64,\n    shuffle=False,\n    num_workers=4\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:32:29.615142Z","iopub.execute_input":"2025-03-03T12:32:29.615441Z","iopub.status.idle":"2025-03-03T12:32:29.619224Z","shell.execute_reply.started":"2025-03-03T12:32:29.615419Z","shell.execute_reply":"2025-03-03T12:32:29.618416Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%%time\n\nfor data in test_dataloader:\n    continue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T11:35:08.163107Z","iopub.execute_input":"2025-03-03T11:35:08.163395Z","iopub.status.idle":"2025-03-03T11:38:28.755575Z","shell.execute_reply.started":"2025-03-03T11:35:08.163375Z","shell.execute_reply":"2025-03-03T11:38:28.754609Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 288 ms, sys: 2.08 s, total: 2.37 s\nWall time: 3min 20s\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"label_to_index = {\n        \"Sunglasses\": 0,\n        \"Sports Shoes\": 1,\n        \"Wallets\": 2,\n        \"Handbags\": 3,\n        \"Kurtas\": 4,\n        \"Casual Shoes\": 5,\n        \"Shirts\": 6,\n        \"Perfume and Body Mist\": 7,\n        \"Tshirts\": 8,\n        \"Tops\": 9,\n        \"Flip Flops\": 10,\n        \"Backpacks\": 11,\n        \"Watches\": 12,\n        \"Belts\": 13,\n        \"Formal Shoes\": 14,\n        \"Socks\": 15,\n        \"Jeans\": 16,\n        \"Heels\": 17,\n        \"Sandals\": 18,\n        \"Briefs\": 19\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:32:33.397670Z","iopub.execute_input":"2025-03-03T12:32:33.397983Z","iopub.status.idle":"2025-03-03T12:32:33.402217Z","shell.execute_reply.started":"2025-03-03T12:32:33.397952Z","shell.execute_reply":"2025-03-03T12:32:33.401319Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"index_to_label = {v: k for k, v in label_to_index.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:32:33.657930Z","iopub.execute_input":"2025-03-03T12:32:33.658210Z","iopub.status.idle":"2025-03-03T12:32:33.662015Z","shell.execute_reply.started":"2025-03-03T12:32:33.658184Z","shell.execute_reply":"2025-03-03T12:32:33.661275Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"models_to_test = []\nfor i in range(k):\n    model = create_model()\n    checkpoint = torch.load(f\"/kaggle/working/efficientnet_b4-fold-{i+1}.pth\")   \n    model.load_state_dict(checkpoint)\n    model.eval()\n    models_to_test.append(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T23:15:40.232957Z","iopub.execute_input":"2025-03-04T23:15:40.233233Z","iopub.status.idle":"2025-03-04T23:15:43.153450Z","shell.execute_reply.started":"2025-03-04T23:15:40.233212Z","shell.execute_reply":"2025-03-04T23:15:43.152790Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-13-13be245731a0>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(f\"/kaggle/working/efficientnet_b4-fold-{i+1}.pth\")\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Предсказание с использованием всех обученных моделей и усреднением их логитов","metadata":{}},{"cell_type":"code","source":"predictions = []\nwith torch.no_grad():\n    for images in test_dataloader:\n        images = images.to(device)\n        outputs0 = models_to_test[0](images)\n        outputs1 = models_to_test[1](images)\n        outputs2 = models_to_test[2](images)\n        outputs3 = models_to_test[3](images)\n        outputs4 = models_to_test[4](images)\n\n        avg_outputs = (outputs0 + outputs1 + outputs2 + outputs3 + outputs4) / k\n        _, predicted = torch.max(avg_outputs, 1)\n        predictions.extend(predicted.cpu().numpy())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_predictions = [index_to_label[pred] for pred in predictions]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:38:59.139270Z","iopub.execute_input":"2025-03-03T12:38:59.139596Z","iopub.status.idle":"2025-03-03T12:38:59.145690Z","shell.execute_reply.started":"2025-03-03T12:38:59.139564Z","shell.execute_reply":"2025-03-03T12:38:59.144892Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/dl-5-image-classification/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:39:00.452730Z","iopub.execute_input":"2025-03-03T12:39:00.453055Z","iopub.status.idle":"2025-03-03T12:39:00.476698Z","shell.execute_reply.started":"2025-03-03T12:39:00.453029Z","shell.execute_reply":"2025-03-03T12:39:00.475861Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df['label'] = text_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:39:01.367232Z","iopub.execute_input":"2025-03-03T12:39:01.367452Z","iopub.status.idle":"2025-03-03T12:39:01.375087Z","shell.execute_reply.started":"2025-03-03T12:39:01.367435Z","shell.execute_reply":"2025-03-03T12:39:01.374164Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"df.to_csv(\"/kaggle/working/sub-1.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:39:02.040548Z","iopub.execute_input":"2025-03-03T12:39:02.040791Z","iopub.status.idle":"2025-03-03T12:39:02.063995Z","shell.execute_reply.started":"2025-03-03T12:39:02.040769Z","shell.execute_reply":"2025-03-03T12:39:02.063205Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}